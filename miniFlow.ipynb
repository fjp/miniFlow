{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniFlow Architecture\n",
    "small tensorFlow clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic node representation class\n",
    "Each node might receive input from multiple other nodes.\n",
    "each node creates a single output, which will likely be passed to other nodes.\n",
    "\n",
    "Contains two lists:\n",
    "- one to store references to the inbound ndoes.\n",
    "- one to store references to the outbound nodes.\n",
    "\n",
    "Each node will eventually calculate a value that represents its output.\n",
    "- initialize the value to None to indicate that it exists but hasn't been set yet.\n",
    "\n",
    "Each node will need to be able to pass values forward and perform backpropagation.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Properties\n",
    "        \n",
    "        # Node(s) from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Node(s) to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # For each inbound Node here, add this Node as an outbound Node to _that_ Node.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "        \n",
    "        \n",
    "        def forward(self):\n",
    "            \"\"\"\n",
    "            Forward propagation.\n",
    "            \n",
    "            Compute the output value vased on `inbound_nodes` and store the result in self.value.\n",
    "            \"\"\"\n",
    "            raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes that Calculate - subclasses\n",
    "While `Node` defines the base set of properties that every node holds, only specialized [subclasses](https://docs.python.org/3/tutorial/classes.html#inheritance) of `Node` will end up in the graph.\n",
    "\n",
    "The following subclasses of `Node` will be created:\n",
    "- `Input`\n",
    "- `Add`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Input` Subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator.\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    # NOTE: Input node is the only node where the value\n",
    "    # may be passed as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should get the value\n",
    "    # of the previous node from self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value is not None:\n",
    "            self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the other subclasses of `Node`, the `Input` subclass does not actually calculate anything. The `Input` subclass just holds a `value`, such as a data feature or a model parameter (weight/bias).\n",
    "\n",
    "The `value` can be either set explicitly or with the `forward()` method. This value is then fed through the rest of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Add` Subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Add`, which is another subclass of `Node`, actually can perform a calculation (addition).\n",
    "\n",
    "Use an [unpacking argument list](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) to accept any number of inputs that are summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, x, y):\n",
    "        # Pass the inbound_nodes list [x, y] to \n",
    "        # the Node instantiator. The Add subclass takes two\n",
    "        # inbound nodes, x and y, and adds the values of those nodes.\n",
    "        Node.__init__(self, [x, y])\n",
    "        \n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, inputs)\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node (`self.value`) to the sum of it's inbound_nodes.\n",
    "        \n",
    "        \"\"\"\n",
    "        sum = 0\n",
    "        for n in self.inbound_nodes:\n",
    "            sum += n.value\n",
    "        self.value = sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Mul` Subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Mul`, which is another subclass of `Node`, actually can perform a calculation (multiplication).\n",
    "\n",
    "Use an [unpacking argument list](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) to accept any number of inputs that are multiplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mul(Node):\n",
    "    def __init__(self, x, y):\n",
    "        # Pass the inbound_nodes list [x, y] to \n",
    "        # the Node instantiator. The Add subclass takes two\n",
    "        # inbound nodes, x and y, and adds the values of those nodes.\n",
    "        Node.__init__(self, [x, y])\n",
    "        \n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, inputs)\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node (`self.value`) to multiply it's inbound_nodes.\n",
    "        \n",
    "        \"\"\"\n",
    "        product = 1\n",
    "        for n in self.inbound_nodes:\n",
    "            product *= n.value\n",
    "        self.value = product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "`MiniFlow` has two methods to help define and then run values through graphs:\n",
    "- `topological_sort()`\n",
    "- `forward_pass()`\n",
    "\n",
    "The picture from Udacity shows an example of topological sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![An example of topological_sort](./figures/topological-sort001.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to define a network, we need to define the order of operations for nodes. Given that the input to some node depends on the outputs of others, we need to flatten the graph in such a way where all the input dependencies for each node are resolved before trying to run its calculation. This is a technique called [topological sort](https://en.wikipedia.org/wiki/Topological_sorting)\n",
    "\n",
    "The `topological_sort()` function implements topological sorting using [Kahn's Algorithm](https://en.wikipedia.org/wiki/Topological_sorting#Kahn.27s_algorithm). \n",
    "\n",
    "`topological_sort()` returns a sorted list of nodes in which all of the calculations can run in series.\n",
    "\n",
    "topological_sort() takes in a feed_dict, which is how we initially set a value for an Input node. The feed_dict is represented by the Python dictionary data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward_pass()` actually runs the network and outputs a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: The output node of the graph (no outgoing edges).\n",
    "        `sorted_nodes`: a topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 + 5 + 10 = 219 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script builds and runs a graph with miniflow.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "x, y, z = Input(), Input(), Input()\n",
    "\n",
    "fAdd = Add(x, y, z)\n",
    "fMul = Mul(x, y, z)\n",
    "f = Add(fAdd, fMul)\n",
    "\n",
    "feed_dict = {x: 4, y: 5, z: 10}\n",
    "\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(f, sorted_nodes)\n",
    "\n",
    "# NOTE: because topological_sort set the values for the `Input` nodes we could also access\n",
    "# the value for x with x.value (same goes for y).\n",
    "print(\"{} + {} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], feed_dict[z], output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Loss\n",
    "\n",
    "## The linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture from Udacity shows a neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Linear function neuron](./figures/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The linear function `Node` aka Neuron depends on three components:\n",
    "- inputs $x_i$,\n",
    "- weights $\\omega_i$\n",
    "- bias $b$\n",
    "\n",
    "The output $y$, is just the weighted sum of the inputs plus the bias.\n",
    "\n",
    "$$\n",
    "y = \\sum_i x_i \\omega_i + b\n",
    "$$\n",
    "\n",
    "by varying the weights, you can vary the amount of influence any given input has on the output. The learning aspect of neural networks takes place during a process known as backpropagation. In backpropogation, the network modifies the weights to improve the network's output accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the linear function `Node` by setting `self.value` to the bias and then loop through the inputs and weights, adding each weighted input to `self.value`. Notice calling `.value` on `self.inbound_nodes[0]` or `self.inbound_nodes[1]` returns a list.\n",
    "\n",
    "The [zip method](https://docs.python.org/3/library/functions.html#zip) was used to get iterators from the iterables inputs and weights. \n",
    "\n",
    "    zip(iter1 [,iter2 [...]]) --> zip object\n",
    "    \n",
    "Return a zip object whose `.__next__()` method returns a tuple where\n",
    "the i-th element comes from the i-th iterable argument.  The `.__next__()`\n",
    "method continues until the shortest iterable in the argument sequence\n",
    "is exhausted and then it raises StopIteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `numpy.array` ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html)) to handle matrices and vectors as input to the `Linear` node. For this `np.dot` will be used to multiply 2D arrays ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)). `numpy` also overloads the `__add__` operator so two arrays can be added directly (eg. `np.array() + np.array()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "        # NOTE: The weights and bias properties here are not\n",
    "        # numbers, but rather references to other nodes.\n",
    "        # The weight and bias values are stored within the\n",
    "        # respective nodes.\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output.\n",
    "        \n",
    "        \"\"\"\n",
    "        # for implementation details see below (section Transform)\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "        \n",
    "        #import pdb \n",
    "        #pdb.set_trace()\n",
    "        #sum = 0\n",
    "        #for i, x in enumerate(self.inbound_nodes[0].value):\n",
    "        #    weight = self.inbound_nodes[1].value\n",
    "        #    sum += x*weight[i]\n",
    "        #b = self.inbound_nodes[2].value\n",
    "        #self.value = sum + b\n",
    "        \n",
    "        #inputs = self.inbound_nodes[0].value\n",
    "        #weights = self.inbound_nodes[1].value\n",
    "        #bias = self.inbound_nodes[2]\n",
    "        #self.value = bias.value\n",
    "        #for x, w in zip(inputs, weights):\n",
    "        #    self.value += x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE: Here an Input node is used for more than a scalar.\n",
    "In the case of weights and inputs the value of the Input node is a python list!\n",
    "\n",
    "In general, there's no restriction on the values that can be passed to an Input node.\n",
    "\"\"\"\n",
    "\n",
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "    inputs: [6, 14, 3],\n",
    "    weights: [0.5, 0.25, 1.4],\n",
    "    bias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Linear function neuron](./figures/transform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear algebra nicely reflects the idea of transforming values between layers in a graph. In fact, the concept of a transform does exactly what a layer should do - it converts inputs to outputs in many dimensions.\n",
    "\n",
    "The output equation for $y$, was the weighted sum of the inputs plus the bias.\n",
    "\n",
    "$$\n",
    "y = \\sum_i x_i \\omega_i + b\n",
    "$$\n",
    "\n",
    "Now the input and the weights are going to be a matrices X and W respectively. The bias b is now a vector instead of a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples on the structure of $X$ and $W$ using the `Linear` node:\n",
    "\n",
    "(In this context an input/output is synonymous with a feature.\n",
    "\n",
    "- If we have a node with one input and k outpus (mapping one input to k outputs) then:\n",
    "\n",
    "    $X$ is a $1 \\times 1$ matrix\n",
    "\n",
    "    $$\n",
    "    X = [X_{11}]\n",
    "    $$\n",
    "    $$\n",
    "    1 \\times 1 \\matrix{ matrix, one element.}\n",
    "    $$\n",
    "    \n",
    "    $W$ becomes a $1 \\times k$ matrix (a row vector)\n",
    "    \n",
    "    $$\n",
    "    W = \\begin{bmatrix} W_{11} & W_{12} & W_{13} & \\ldots & W_{1k} \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    1 \\times k \\text{ weights matrix, row matrix/vector.}\n",
    "    $$\n",
    "    \n",
    "    The result of the matrix multiplication of $X$ and $W$ is a $1 \\times k$ matrix. Since $b$ is also a $1 \\times k$ row matrix (1 bias per output), $b$ is added to the output of the $X$ and $W$ matrix multiplication.\n",
    "    \n",
    "    \n",
    "- If we map n inputs to k outpus (mapping multiple inputs to k outputs) then:\n",
    "\n",
    "    $X$ is now a $1 \\times n$ matrix and $W$ is a $n \\times k$ matrix. The result of the matrix multiplication is still a $1 \\times k$ matrix so the use of the biases remain the same.\n",
    "    \n",
    "    $$\n",
    "    X = \\begin{bmatrix} X_{11} & X_{12} & X_{13} & \\ldots & X_{1n} \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    1 \\times n \\text{ input matrix, n inputs/features.}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    W = \\begin{bmatrix} W_{11} & W_{12} & W_{13} & \\ldots & W_{1k} \\\\\n",
    "                        W_{21} & W_{22} & W_{23} & \\ldots & W_{2k} \\\\\n",
    "                        W_{31} & W_{32} & W_{33} & \\ldots & W_{3k} \\\\\n",
    "                        \\ldots &        &        & \\ddots &        \\\\\n",
    "                        W_{n1} & W_{n2} & W_{n3} & \\ldots & W_{nk} \\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    1 \\times k \\text{ weights matrix, row matrix/vector.}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    b = \\begin{pmatrix} b_{1} & b_{2} & X_{3} & \\ldots & b_{k} \\end{pmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    1 \\times k \\text{row vector of biases, one for each output.}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of n inputs would be a 28px by 28px greyscale image, as is in the case of images in the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a set of handwritten digits. We can reshape the image such that it's a 1 by $784$ matrix, $n = 784$. Each pixel is an input/feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it's common to feed in multiple data examples in each forward pass rather than just one. The reasoning for this is the examples can be processed in parallel, resulting in big performance gains. The number of examples is called the batch size. Common numbers for the batch size are 32, 64, 128, 256, 512. Generally, it's the most we can comfortably fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that $X$ becomes a $m \\times n$ matrix and $W$ and $b$ remain the same. The result of the matrix multiplication is now $m \\times k$, so the addition of $b$ is [broadcast](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) over each row.\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} X_{11} & X_{12} & X_{13} & \\ldots & X_{1n} \\\\\n",
    "                    X_{21} & X_{22} & X_{23} & \\ldots & X_{2n} \\\\\n",
    "                    X_{31} & X_{32} & X_{33} & \\ldots & X_{3n} \\\\\n",
    "                    \\ldots &        &        & \\ddots &        \\\\\n",
    "                    X_{m1} & X_{m2} & X_{m3} & \\ldots & X_{mn} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "X\\text{ is now an } m \\times n \\text{ matrix. Each row has n inputs/features.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of MNIST each row of $X$ is an image reshaped from $28$ by $28$ to $1 \\times 784$.\n",
    "\n",
    "\n",
    "$$\n",
    "Z = XW + b\n",
    "$$\n",
    "\n",
    "This equation can also be views as $Z = XW + B$ where $B$ is the biases vector, $b$, stacked $m$ times as a row. Due to broadcasting it's abbreviated to $Z = XW + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.  4.]\n",
      " [-9.  4.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code test section\n",
    "\"\"\"\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Function\n",
    "\n",
    "Neural networks take advantage of alternating transforms and activation functions to better categorize outputs. The sigmoid function is among the most common activation functions.\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logistic curve](./figures/logistic-curve.svg)\n",
    "Graph of the sigmoid funciton. Notice the \"S\" shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear transforms are great for simply shifting values, but neural networks often require a more nuanced transform. For instance, one of the original designs for an artificial neuron, the [perceptron](https://en.wikipedia.org/wiki/Perceptron), exhibit binary output behavior. Perceptrons compare a weighted input to a threshold. When the weighted input exceeds the threshold, the perceptron is **activated** and outputs `1`, otherwise it outputs `0`.\n",
    "\n",
    "The behavior of a perceptron can be modeled as a step function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![step function](./figures/step.png)\n",
    "Example of a step function (The jump between $y = 0$ and $y = 1$ should be instantaneous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation, the idea of binary output behavior, generally makes sense for classification problems. For example, if you ask the network to hypothesize if a handwritten image is a '9', you're effectively asking for a binary output - yes, this is a '9', or no, this is not a '9'. A step function is the starkest form of a binary output, which is great, but step functions are not continuous and not differentiable, which is very bad. Differentiation is what makes gradient descent possible.\n",
    "\n",
    "The sigmoid function $\\sigma(x)$, replaces thresholding with a beautiful S-shaped curve (shown above) that mimics the activation behavior of a perceptron while maintaining continuity, and thus differentiability. As a bonus, the sigmoid function has a very simple derivative that looks remarkably similar to the sigmoid itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the sigmoid function only has one parameter. Remember that sigmoid is an activation function (non-linearity), meaning it takes a single input and performs a mathematical operation on it.\n",
    "\n",
    "Conceptually, the sigmoid function makes decisions. When given weighted features from some data, it indicates whether or not the features contribute to a classification. In that way, a sigmoid activation works well following a linear transformation. As it stands right now with random weights and bias, the sigmoid node's output is also random. The process of learning through backpropagation and gradient descent, modifies the weights and bias such that activation of the sigmoid node begins to match expected outputs.\n",
    "To add the equation of the sigmoid function to the `MiniFlow` library we use `np.exp` ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `Sigmoid` in conjunction with `Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    You need to fix the `_sigmoid` and `forward` methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used later with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "\n",
    "        Return the result of the sigmoid function.\n",
    "\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the result of the\n",
    "        sigmoid function, `_sigmoid`.\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This network feeds the output of a linear transform\n",
    "to the sigmoid function.\n",
    "\n",
    "Finish implementing the Sigmoid class in miniflow.py!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the derivative of the sigmoid function, the sigmoid function is actually a part of its own derivative. Keeping _sigmoid separate means we won't have to implement it twice for forward and backward propagations.\n",
    "\n",
    "We have used weights and biases to compute outputs. And we used an activation function to categorize the output. Neural networks improve the accuracy of their outputs by modifying weights and biases in response to training against labeled datasets.\n",
    "There are many techniques for defining the accuracy of a neural network, all of which center on the network's ability to produce values that come as close as possible to known correct values. People use different names for this accuracy measurement, often terming it loss or cost.\n",
    "\n",
    "We will calculate the cost using the mean squared error (MSE):\n",
    "\n",
    "$$\n",
    "C(w, b) = \\frac{1}{m} \\sum_x (y(x) - a)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $w$ denotes the collection of all weights in the network, $b$ all the biases, m is the total number of training examples, $a$ is the approximation of $y(x)$ by the network, both a and $y(x)$ are vectors of the same length.\n",
    "\n",
    "The collection of weights is all the weight matrices flattened into vectors and concatenated to one big vector. The same goes for the collection of biases except they're already vectors so there's no need to flatten them prior to the concatenation.\n",
    "\n",
    "Here's an example of creating w in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 by 2 matrices\n",
    "w1  = np.array([[1, 2], [3, 4]])\n",
    "w2  = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# flatten\n",
    "w1_flat = np.reshape(w1, -1)\n",
    "w2_flat = np.reshape(w2, -1)\n",
    "\n",
    "w = np.concatenate((w1_flat, w2_flat))\n",
    "# array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a nice way to abstract all the weights and biases used in the neural network and makes some things easier to write as we'll see soon in the upcoming gradient descent sections.\n",
    "\n",
    "The cost, $C$, depends on the difference between the correct output, $y(x)$, and the network's output, $a$. It's easy to see that no difference between $y(x)$ and $a$ (for all values of $x$) leads to a cost of 0.\n",
    "\n",
    "This is the ideal situation, and in fact the learning process revolves around minimizing the cost as much as possible.\n",
    "\n",
    "As it stands right now, it outputs gibberish. The activation of the sigmoid node means nothing because the network has no labeled output against which to compare. Furthermore, the weights and bias cannot change and learning cannot happen without a cost.\n",
    "\n",
    "We implement the cost function using the `MSE` method ([link](https://en.wikipedia.org/wiki/Mean_squared_error)) and `np.square` ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.square.html)). To reshape the weights we use reshape ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        \n",
    "        diff = y - a\n",
    "        self.value = np.mean(diff**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test MSE method with this script!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(cost, graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math behind MSE reflects the equation of the cost function above, where $y$ is target output and $a$ is output computed by the neural network. We then square the difference `diff**2`, alternatively, this could be `np.square(diff)`. Lastly we need to sum the squared differences and divide by the total number of examples m. This can be achieved in with `np.mean` or `(1 /m) * np.sum(diff**2)`.\n",
    "\n",
    "Note the order of $y$ and $a$ doesn't actually matter, we could switch them around $(a - y)$ and get the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
